import argparse

def parser_train():
    parser = argparse.ArgumentParser(description='Wasserstein Multi-modal')
    parser.add_argument('--scenario', type=str, default="AntCustom-v0", help="environment")
    # parser.add_argument('--num_episodes', type=int, default=5000, help="number of episodes for training")
    # parser.add_argument('--max_episode_len', type=int, default=200, help="maximum episode length")
    # parser.add_argument('--hidden_dim', type=int, default=256, help="network hidden size")
    # parser.add_argument('--buffer_limit', type=int, default=1000000, help="an imagined limit for replay buffer")
    # parser.add_argument('--log_interval', type=int, default=20, help="calculate avg reward every log_interval episodes")
    # parser.add_argument('--updates_per_step', type=int, default=1, help='model updates per simulator step (default: 1)')
    # parser.add_argument('--start_steps', type=int, default=20000, help='Steps sampling random actions (default: 20000)')
    # parser.add_argument('--target_update_interval', type=int, default=1, help='Value target update per no. of updates per step (default: 1)')
    parser.add_argument('--seed', type=int, default=123, help="random seed for env")
    # parser.add_argument('--tau', type=float, default=0.005, help='target smoothing coefficient(τ) (default: 0.005)')
    # parser.add_argument('--beta1', type=float, default=0.9, help="beta1 for adam optimizer")
    # parser.add_argument('--beta2', type=float, default=0.999, help="beta2 for adam optimizer")
    # parser.add_argument('--beta', type=float, default=0., help="beta for entropy term")
    # parser.add_argument('--gamma', type=float, default=0.999, help="discounted factor")
    # parser.add_argument('--critic_lr', type=float, default=0.0003, help="lr for the critic")
    # parser.add_argument('--policy_lr', type=float, default=0.0003, help="lr for the policy")
    # parser.add_argument('--alpha', type=float, default=0.2, help='Temperature parameter α determines the relative importance of the entropy\
    #                         term against the reward (default: 0.2)'                                                                                                                                                                                                                                                                                                                                                                                          )
    # parser.add_argument('--automatic_entropy_tuning', type=bool, default=False, help='Automaically adjust α (default: False)')
    # parser.add_argument('--batch_size', type=int, default=128, help="maximum number of steps retrieved from buffer")
    # parser.add_argument('--disc_batch_size', type=int, default=200, help="batch size for predictor")
    # parser.add_argument('--pred_lr', type=float, default=0.0003, help="learning rate for predictor")
    # parser.add_argument('--disc_lr', type=float, default=0.0003, help="learning rate for discriminator")
    parser.add_argument('--cuda', action='store_false', help='run on GPU (default: True)')
    parser.add_argument('--policy', type=str, default="Gaussian", help="policy type for backend")
    parser.add_argument('--num_modes', type=int, default=10, help="number of modes that need to learn")
    # parser.add_argument('--reward_scale', type=float, default=100, help="scale factor for pseudo reward")
    parser.add_argument('--algo', type=str, default='sac', help="training algorithm for agent learning")
    parser.add_argument('--sr_algo', type=str, default='apwd', help="algorithm for pseudo reward computing")
    parser.add_argument('--schedule', type=str, default='random', help="learning schedule for policy modes('random', 'even')")
    parser.add_argument('--exp_name', type=str, default="wurl", help="The name of the experiment('wurl', 'dads', 'diayn')")
    parser.add_argument('--run', type=str, default="10086", help="index of individual runs")
    return parser

def parser_test():
    parser = argparse.ArgumentParser(description='Wasserstein Multi-modal Tests')
    parser.add_argument('--scenario', type=str, default="AntCustom-v0", help="environment")
    # parser.add_argument('--test_episodes', type=int, default=50, help="number of episodes for testing")
    # parser.add_argument('--max_episode_len', type=int, default=200, help="maximum episode length")
    # parser.add_argument('--hidden_dim', type=int, default=256, help="network hidden size")
    # parser.add_argument('--updates_per_step', type=int, default=1, help='model updates per simulator step (default: 1)')
    # parser.add_argument('--target_update_interval', type=int, default=1, help='Value target update per no. of updates per step (default: 1)')
    # parser.add_argument('--seed', type=int, default=123, help="random seed for env")
    parser.add_argument('--cuda', action='store_false', help='run on GPU (default: True)')
    # parser.add_argument('--disc_batch_size', type=int, default=200, help="training batch size for discriminators")
    # parser.add_argument('--policy', type=str, default="Gaussian", help="policy type for backend")
    parser.add_argument('--num_modes', type=int, default=10, help="number of modes that need to learn")
    # parser.add_argument('--buffer_limit', type=int, default=10000, help="an imagined limit for replay buffer")
    # parser.add_argument('--reward_scale', type=float, default=1e4, help="scale factor for pseudo reward")
    parser.add_argument('--algo', type=str, default='sac', help="training algorithm for agent learning")
    # parser.add_argument('--wde_algo', type=str, default='apwd', help="algorithm for pseudo reward computing")
    parser.add_argument('--backend', type=str, default='apwd', help="Backend of learned policies(apwd, diayn, dads)")
    parser.add_argument('--prefix', type=str, default='models/AntCustom-v0/wurl/apwd/run', help="model loading path prefix")
    parser.add_argument('--skill_run', type=str, default="10086", help="index of the skill training runs")
    # parser.add_argument('--run', type=str, default="10086", help="index of individual runs")
    return parser


def parser_hrl():
    parser = argparse.ArgumentParser(description='Wasserstein Multi-modal HRL')
    parser.add_argument('--scenario', type=str, default="AntCustom-v0", help="environment")
    parser.add_argument('--subscenario', type=str, default="AntCustom-v0", help="sub environment")
    parser.add_argument('--max_episode_len', type=int, default=50, help="maximum episode length")
    parser.add_argument('--max_subepisode_len', type=int, default=20, help="maximum episode length for lower policy")
    # parser.add_argument('--hidden_dim', type=int, default=64, help="network hidden size")
    # parser.add_argument('--updates_per_step', type=int, default=1, help='model updates per simulator step (default: 1)')
    # parser.add_argument('--target_update_interval', type=int, default=1, help='Value target update per no. of updates per step (default: 1)')
    parser.add_argument('--seed', type=int, default=123, help="random seed for env")
    parser.add_argument('--cuda', action='store_false', help='run on GPU (default: True)')
    parser.add_argument('--disc_batch_size', type=int, default=200, help="training batch size for discriminators")
    parser.add_argument('--policy', type=str, default="Gaussian", help="policy type for backend")
    parser.add_argument('--num_modes', type=int, default=10, help="number of modes that need to learn")
    parser.add_argument('--buffer_limit', type=int, default=100000, help="an imagined limit for replay buffer")
    parser.add_argument('--reward_scale', type=float, default=1e4, help="scale factor for pseudo reward")
    # parser.add_argument('--algo', type=str, default='sacd', help="training algorithm for agent learning")
    parser.add_argument('--backend', type=str, default='apwd', help="Backend of lower policies(apwd, diayn, dads)")
    parser.add_argument('--prefix', type=str, default='models/AntCustom-v0/wurl/apwd/run1/', help="model loading path prefix")
    parser.add_argument('--skill_run', type=str, default="10086", help="index of the skill training runs")
    parser.add_argument('--run', type=str, default="10086", help="index of individual runs")
    return parser
